 
# What is the difference between EM and Gradient Ascent?
https://www.zhihu.com/question/20993648

From:

> Xu L and Jordan MI (1996). On Convergence Properties of the EM Algorithm for Gaussian Mixtures. Neural Computation 2: 129-151.

Abstract:

> We show that the EM step in parameter space is obtained from the gradient via a projection matrix P, and we provide an explicit expression for the matrix.

Page 2

> In particular we show that the EM step can be obtained by pre-multiplying the gradient by a positive denite matrix. We provide an explicit expression for the matrix ...

Page 3

> That is, the EM algorithm can be viewed as a variable metric gradient ascent algorithm ...

This is, the paper provides explicit transformations of the EM algorithm into gradient-ascent, Newton, quasi-Newton.

**------------------------------------**
If you are interested in an optimization point-of-view on EM, in this paper you will see that EM algorithm is a special case of wider class of algorithms (proximal point algorithms).

ON EM ALGORITHMS AND THEIR PROXIMAL GENERALIZATIONS

**------------------------------------**
Manifold Optimization for Gaussian Mixture Models



# 如何才能看得懂变分贝叶斯方法

https://www.zhihu.com/question/20993648

Variational Inference: A Review for Statisticians
